<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-15">

  
    <title>Trabalho Prático 1</title>
  </head><body>
<h1> Trabalho Prático 1 </h1>
     <h2> (duração: 5 aulas)</h2>

O objectivo deste trabalho é ilustrar a matéria dada nas aulas teóricas
sobre os gestores de recursos e em particular o uso do sistema
<a href="http://www.cs.wisc.edu/condor/">Condor</a>.

Pretende-se também a revisão/introdução aos modelos de programação paralela distribuída, em particular pela resolução de um problema com recurso a um middleware para troca de mensagens (o MPI).

<h2> O problema a resolver </h2>

<p>
Os sistemas de procura (ou busca) de informação baseiam-se muitas vezes na existência de índices remissivos que referenciam a localização da informação a procurar dentro de todos os dados disponíveis. Este mesmo principio suporta, por exemplo, os motores de busca de texto na internet, ou em documentos arquivados numa biblioteca. 
</p>
<p>
Pretende-se então construir o índice remissivo (inverted index) para todas as palavras de um conjunto de documentos, estando cada documento num ficheiro. Pode-se imaginar aplicar a sua solução a um conjunto de milhões de documentos (ou a toda a internet?) e ser capaz de efectuar buscas que devolvam os documentos onde determinadas palavras aparecem, ordenados por um determinado critério de relevância, por exemplo o número de ocorrências das palavras procuradas nos documentos ou o 'Page-rank' usado pela Google[1].
</p>
<p>
Comece por desenvolver um programa (recomenda-se C) que permita obter o histograma de ocorrências de todas as palavras relevantes de um documento (identifique as palavras que deve ignorar). Desenvolva outro que permita fundir os vários índices, obtidos a partir de cada documento, num único indice global que agregue a informação sobre as ocorrências de cada palavra em cada documento.
</p>
<p>
Dado um conjunto de ficheiros com os documentos a processar, construa uma solução para a execução dos seus programas que calculam os indices parciais e depois o global, tirando partido do sistema Condor sobre o cluster de computadores usado no laboratório. Desenvolva também uma aplicação baseada no middleware MPI onde o seu programa obtém os indices parciais em paralelo e depois os junta no único indice global (ver mais à frente). Esta segunda solução deve também ser executada usando o Condor e ambas as soluçãoes devem ser comparadas. 
</p>

<h2> O sistema Condor </h2>

<p>A descrição dos princípios gerais e das funcionalidades deste
sistema já foi feita e pode ser encontrada em vários artigos
assim como em:
<a href="http://asc.di.fct.unl.pt/%7Evad/SCG/condor-overview.pdf">Condor overview</a> e 
<a href="http://asc.di.fct.unl.pt/%7Evad/SCG/condor-chapter.pdf">Condor - A distributed job scheduler</a>.
No cluster a usar encontra-se instalada a versão (6.8.6) cuja documentação pode ser consultada em
 <a href="http://asc.di.fct.unl.pt/%7Evad/SCG/condor-V6_8_6-Manual.pdf">manual (PDF)</a>.
</p>

<p>
Para submeter um programa para execução no condor é necessário escrever
um ficheiro que descreve o trabalho. Exemplo:</p>
<pre># neste exemplo executa uma única instancia de um programa chamado 'myapp'
Universe = vanilla
Executable = myapp
Arguments = arg1 arg2
Input = myapp.in
Output = myapp.out
Error = myapp.error
Log = myapp.log
should_transfer_files = NO
Queue 1
</pre>

<p>Supondo que o ficheiro acima se chama <em>myapp.condor</em>, o trabalho é 
submetido para execução com o comando:</p>
<pre>condor_submit myapp.condor
</pre>

<p>Veja o manual: <a href="http://www.cs.wisc.edu/condor/manual/v6.8/condor_submit.html">condor_submit </a></p>

<p>Pode-se ver o estado da fila de execução com o comando:
</p>
<pre>condor_q
</pre>
<p>O estado das máquinas geridas pelo Condor pode ser visto com o comando:
</p>
<pre>condor_status
</pre>

<p>Podemos descrever a execução de vários processos, pela repetição de entradas no ficheiro que descreve o
 trabalho e/ou pelo número indicado na directiva "Queue". Estes processos serão criados à medida que existam nós disponíveis.
Os processos criados nas várias máquinas são numerados segundo a regra <em>cluster_number.process_number</em> em que:
</p>
<ul>
<li> cluster_number: é um número inteiro positivo que está associado a um 
comando <em>condor_submit</em> efectuado por um utilizador. Este número é
distinto para cada submissão e cresce monotonamente. Pode ser usado dentro
do ficheiro de submissão recorrendo à variável $(cluster).
</li>
<li> process_number: corresponde a cada processo criado dentro do mesmo trabalho que é argumento do comando <em>condor_submit</em>. Começa em 0 e vai crescendo
sendo acessível através da variável $(process).
</li></ul>

<p>Um trabalho ou <em>cluster</em> (ou que ainda resta dele) pode ser removido da fila:
</p>
<pre>condor_rm cluster_number
</pre>

<p>Um determinado <em>process</em> pode ser removido da fila de trabalhos submetidos:
</p>
<pre>condor_rm cluster_number.process_number
</pre>

<h3> O condor e os ficheiros de I/O</h3>

<p>Na actual configuração o condor tem o gestor no nó <em>bldin01</em> que é
também a máquina de submissão. Os nós de execução são os <em>bldin02</em>
a <em>bldin06</em>. Estes partilham o sistema de ficheiros o que significa que todos os processos podem aceder aos mesmos ficheiros, independentemente do computador onde executam. Tenha assim em atenção que, se mais de um processo escrever no mesmo ficheiro,
estes vão destruindo as escritas uns dos outros. Para indicar ficheiros diferentes da descrição do trabalho use as variáveis $(process) nos nomes dos ficheiros.</p>

<p>Quando não existe um sistema de ficheiros partilhado, levanta-se 
a questão de como são transferidos os ficheiros de entrada e saída de cada processo.
Nestes casos será preciso indicar ao Condor que é preciso transferir os ficheiros entre o nó de submissão e os de execução; também é necessário
indicar em que altura é feita a transferência do ficheiro de saída
da máquina de execução para a máquina onde foi feita a submissão (normalmente
apenas no final). Isto consegue-se acrescentando o seguinte ao ficheiro
de submissão:
<pre>ShouldTransferFiles = YES
WhenToTransferOutput = ON_EXIT
</pre>


<h2> Fluxo de trabalho (Workflows &amp; DAG) </h2>

<p>Toda a experiência de simulação descrita na primeira aula, traduz-se numa sequência de tarefas, onde existem interdependências. Por exemplo, só se pode fazer o gráfico da relação entre a probabilidade de propagação do fogo e a área ardida depois de efectuar todas as simulações. Este tipo de encadeamento de tarefas, pode ser modelado como um fluxo de trabalho (workflow) e ser representado por um grafo orientado sem ciclos (Directed Acyclic Graph ou DAG). Um exemplo simples pode ser o seguinte:
</p>
<p><IMG src="SimpleDag.gif" width="480" height="267" align="center" border="0"></p>


<p>
O Condor suporta a descrição destas dependências entre tarefas (jobs) e permite submeter automaticamente vários jobs de acordo com a descrição das suas dependências. Para tal é necessário escrever, para além das varias descrições dos diferentes jobs como visto na aula anterior, um novo ficheiro que descreve as suas dependências. O exemplo anterior, mas considerando apenas 3 instancias do "Analyze", pode ser descrito da seguinte forma: </p>
<pre>
Job Prepare prepare.condor
Job Analyze1 analyze1.condor
Job Analyze2 analyze2.condor
Job Analyze3 analyze3.condor
Job Finalize finalize.condor

PARENT Prepare CHILD Analyze1 Analyze2 Analyze3
PARENT Analyze1 Analyze2 Analyze3 CHILD Finalize
</pre>

<p>Supondo que os vários ficheiros acima da forma *.condor descrevem cada nó do grafo e que o ficheiro acima se chama <em>myapp.dag</em>, todo o trabalho é 
submetido para execução com o comando:</p>
<pre>condor_submit_dag myapp.dag
</pre>

<p>Veja o manual: <a href="http://www.cs.wisc.edu/condor/manual/v6.8/condor_submit_dag.html">condor_submit_dag </a></p>
<p>Ver o tutorial: <A href="http://www.cs.wisc.edu/condor/tutorials/intl-grid-school-3/simple_dag.html">DAGMan</A></p>

<p>Pode-se ver o estado da fila de execução com o comando:
</p>
<pre>condor_q
</pre>
<p>Deve observar que é lançado um processo gestor do workflow que vai, por sua vez, submetendo ao condor os diferentes trabalhos (prepare.condor, analyzeN.condor e finalize.condor) de acordo com as dependências expressas no myapp.dag.
</p>

<h2> A norma MPI (Message Passing Interface) </h2>

<p>
O <a href="http://www.mpi-forum.org"> MPI Forum </a> é uma organização que
agrega fabricantes e instituições de ensino e investigação e que se tem
dedicado a definir uma norma aplicável a bibliotecas de programação paralela
baseadas em troca de mensagens. Os documentos produzidos estão acessíveis
no "site" acima referido.

<p>
As especificações publicadas têm sido respeitadas pelas entidades
que têm desenvolvido essas bibliotecas, pelo que a norma MPI se tornou
um "standard" de facto, sendo hoje usada para quase todo o desenvolvimento
de aplicações paralelas que se baseiam em troca de mensagens.

<p>
A especificação engloba um grande número de primitivas que tem vindo a 
crescer ao longo dos anos. A versão actual é a MPI-2, mas nem todas as
implementações a suportam completamente; para a maior parte das
aplicações é suficiente usar as funções definidas pela versão MPI-1.

<p>
Para uma introdução ao uso do MPI (usando a linguagem C) pode consultar
os seguintes documentos:
</p>
<ul>
<li> <a href="http://www.llnl.gov/computing/tutorials/mpi/"> MPI Tutorial - Lawrence Livermore National Laboratory (HTML) </a>
<li> <a href="./mpi.guide.pdf">A User's Guide to MPI - P.S. Pacheco, Univ. San Francisco (PDF)</a>
<li> <a href="http://www-unix.mcs.anl.gov/mpi/tutorial/"> MPI Tutorial - Argonne National Laboratory (HTML) </a>
</ul>

<!--
<h2> LAM/MPI</h2>

<p> 
Vamos usar uma implementação de MPI desenvolvida na Universidade de
Ohio chamada <em>LAM/MPI</em>. Informação sobre esta pode ser obtida
em <a href="http://www.lam-mpi.org">LAM/MPI Parallel Computing</a>.

<p>
Antes de se poder correr um programa MPI é preciso fazer o <em>arranque</em>
da máquina virtual em que a aplicação é executada. Para isso é necessário
escrever um ficheiro de configuração que vamos chamar <em>my-hosts</em>:</p>
<pre>
bldin01 cpu=2
bldin02 cpu=2
bldin03 cpu=2
bldin04 cpu=2
bldin05 cpu=2
bldin06 cpu=2
</pre>

<p>
Para arrancar a máquina virtual é preciso fazer:
</p>
<pre>
lamboot my-hosts
</pre>

<p>
No final da sessão, devemos desligar a máquina virtual:</p>
<pre>
lamhalt
</pre>

<p>
Mais detalhes sobre estes comandos e outros podem ser obtidos em:
</p>
<ul>
<li> <a href="http://www.lam-mpi.org/tutorials/one-step/lam.php">One-Step Tutorial: Getting started with LAM</a>
<li> <a href="http://www.lam-mpi.org/tutorials/lam/">MPI Tutorials: Getting started with LAM/MPI</a>
</ul>

-->


<p>
Um exemplo de um programa que usa o MPI é o seguinte:
</p>
<pre>
#include &lt;mpi.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

main( int argc, char **argv){
      int myrank;
      int len;

      char name [MPI_MAX_PROCESSOR_NAME];

      MPI_Init(&argc, &argv);
      MPI_Comm_rank( MPI_COMM_WORLD, &myrank);
      MPI_Get_processor_name( name, &len ); 

      printf("%s: hello from %d\n", name, myrank);

      MPI_Finalize();
} </pre>

Para compilar um programa e permitir a execução em todos os nós, as
acções recomendadas estão resumidas no <em>Makefile</em> seguinte:
<pre>
CC = mpicc
CFLAGS = -g -Wall
TARGET = exemplo
INSTALLDIR = ~/bin

all: $(TARGET) install

$(TARGET): $(TARGET).c
	$(CC) $(CFLAGS) -o $(TARGET) $&lt;

install:
	cp $(TARGET) $(INSTALLDIR)

clean:
	rm -f *.o $(TARGET) $(INSTALLDIR)/$(TARGET)

</pre>

<!--A compilação deve ser feita no nó <em>bldin01</em>.-->
Note que no objectivo <em>install</em> o executável é copiado para a
directoria <em>~/bin</em> ficando acessível de qualquer directoria, admitindo que esta directoria faz parte do seu PATH, para que o programa possa ser facilmente executado. Se não for assim, pode acrescentar essa directoria ao seu PATH colocando em ~/.bashrc:
<pre>
PATH=$PATH:~/bin
</pre>

<p>
Para facilitar a execução de um programa em vários computadores (nós) é conveniente escrever um ficheiro de configuração, contendo os nós a usar, por exemplo contendo:</p>
<pre>
host01 slots=2
host02 slots=2
host03 slots=2
host04 slots=2
host05 slots=2
</pre>


<p>
Para lançar a aplicação usa-se o comando <em>mpirun</em>. Este comando tem
formatos apresentados a seguir: <em>mpirun -np num_processos -hostfile myhosts programa</em>
</p>
<p> No exemplo seguinte, lançam-se 10 cópias
do programa <em>exemplo</em> deixando ao cuidado do sistema de suporte à execução
a escolha dos nós em que os 10 processos são lançados, de entre os indicados no ficheiro 'myhosts':</p>
<pre>
  mpirun -np 10 -hostfile myhosts exemplo
</pre>

Note-se que neste caso, não estamos a indicar onde estão os executáveis ...
Outra forma será indicar uma configuração para a aplicação. Veja os manuais.


<p>
Para poder aceder remotamente  &eacute; preciso um <em>username</em> e uma
<em>password</em>  o que nem sempre se revela prático, em particular se estiver a usar um cluster com centenas de nós.
Para uma utiliza&ccedil;&atilde;o mais c&oacute;moda conv&eacute;m que entre os seus v&aacute;rios
n&oacute;s se possa fazer "login" usado o <em>ssh</em>
sem ser necess&aacute;rio
introduzir a "password". Para tal sugere-se o uso de chaves assimétricas. Para conseguir isso, execute o seguinte
comando que permite gerar um par de chaves RSA a usar nos logins em vez das
passwords:
</p>
<pre>    ssh-keygen -t rsa</pre>

<p>N&atilde;o d&ecirc; qualquer password. Depois s&oacute; tem de
copiar a sua chave p&uacute;blica, em .ssh/id_rsa.pub, para
.ssh/authorized_keys (tal funciona porque a sua área é montada em todos os nós, logo, todos partilham o ficheiro .ssh/authorized_keys).</p>


<h2> Usando o MPI </h2>

<p>
Usando uma estratégia baseada em master-workers, desenvolva uma solução onde o master vai distribuindo por cada worker os nomes dos ficheiros a processar. No fim efectua a fusão de todos os índices parciais no global.
</p>



<h2>Aplicações MPI em Condor</h2>

<p>Para além do universo "vanilla" anterior, o Condor inclui outros oferecendo um 
suporte diferenciado, para aplicações com outras arquitecturas. Um caso é o universo "parallel" 
que permite lançar aplicações baseadas em "middlewares" de suporte ao paralelismo como o MPI.
 Neste caso a aplicação só pode ser executada quando é garantido o ambiente de suporte à sua execução
 com o número de nós pretendido. 
 A aplicação é então executada criando o ambiente necessário ao MPI,  
e todos os processos necessários são depois lançados pelo ambiente MPI,
 no caso, usando o mpirun. </p>

<p>Assim o universo "parallel" permite a afectação de recursos (CPU) e o lançamento da aplicação
 de forma diferente da antes experimentada, permitindo cumprir os requisitos do MPI. Toda a
 aplicação MPI é vista pelo Condor como um único job. Este só pode ser executado pela afectação, 
em simultâneo, do número de CPU requeridos no ficheiro de descrição do job. Isto significa, como
 esperado, que mesmo que existam CPU disponíveis uma aplicação MPI não pode ir executando nesses
 parte dos processos e deixar outros processos para quando outros CPU ficarem disponíveis. 
Quando o número de CPU requeridos fica disponível, é então lançada a aplicação pela execução de um
 script especial, responsável pelo lançamento do MPI, seguido da execução do mpirun.</p>

<p>Um exemplo da descrição de um trabalho MPI para o Condor é o seguinte:</p>

<pre>
# neste exemplo executa uma aplicacao MPI em 5 CPUs
Executable = /home/condor/libexec/lamscript
machine_count = 5
arguments = myapp
Output = myapp.out.$(node)
Error = myapp.error.$(node)
Log = myapp.log
Queue 1
</pre>

<p> Note que o número de nós a usar pelo MPI é indicado por "machine_count" e o programa
 é dado como argumento (arguments) ao script lamscript (indicado em Executable). É este script que instancia todo o ambiente do MPI
 e depois efectua o lançamento da aplicação com o mpirun.
</p>

<p>Com base no exemplo anterior execute a versão MPI  por sí desenvolvida 
sob controlo do Condor, permitindo a fácil partilha/exclusão do cluster com os seus colegas de aula.</p>


<hr/>
[1] Brin, S.; Page, L.  "The anatomy of a large-scale hypertextual Web search engine". Computer Networks and ISDN Systems 30: pp. 107--117. 1998. doi:10.1016/S0169-7552(98)00110-X. ISSN 0169-7552
</body>
</html>
